{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setting the enviroment"
      ],
      "metadata": {
        "id": "3ePq211fGdTL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVVc5ztpF_Lw"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# 1. Install Unsloth & Dependencies\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes pandasy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip llama3_basketball_adapter.zip -d llama3_basketball_adapter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SB9f7x_ZWk6H",
        "outputId": "fa382736-105a-4348-e73a-f15db3e7b671"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  llama3_basketball_adapter.zip\n",
            "replace llama3_basketball_adapter/README.md? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: llama3_basketball_adapter/README.md  \n",
            "replace llama3_basketball_adapter/special_tokens_map.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: llama3_basketball_adapter/special_tokens_map.json  \n",
            "replace llama3_basketball_adapter/adapter_config.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: llama3_basketball_adapter/adapter_config.json  \n",
            "replace llama3_basketball_adapter/tokenizer.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: llama3_basketball_adapter/tokenizer.json  \n",
            "replace llama3_basketball_adapter/adapter_model.safetensors? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: llama3_basketball_adapter/adapter_model.safetensors  \n",
            "  inflating: llama3_basketball_adapter/tokenizer_config.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting the models"
      ],
      "metadata": {
        "id": "x3ej5rbnGol_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "print(torch.cuda.is_available() and torch.cuda.device_count() > 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGp5LJbpI6Jr",
        "outputId": "ec7d2531-bc1e-4841-9286-ba43cf7a5901"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "ADAPTER_PATH = \"llama3_basketball_adapter\"\n",
        "BASE_MODEL_NAME = \"unsloth/llama-3-8b-bnb-4bit\""
      ],
      "metadata": {
        "id": "YdV__MJ6GrKn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Cases"
      ],
      "metadata": {
        "id": "TWPCGaoUG09G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_cases = [\n",
        "    # --- CLUTCH MOMENTS ---\n",
        "    {\n",
        "        \"id\": 1,\n",
        "        \"input_json\": '{\"time\": \"00:01\", \"team\": \"Fenerbah√ße\", \"player\": \"Nigel Hayes-Davis\", \"action\": \"3pt_shot\", \"result\": \"make\"}'\n",
        "    },\n",
        "    {\n",
        "        \"id\": 2,\n",
        "        \"input_json\": '{\"time\": \"00:00\", \"team\": \"Anadolu Efes\", \"player\": \"Shane Larkin\", \"action\": \"drive_layup\", \"result\": \"make\"}'\n",
        "    },\n",
        "\n",
        "    # --- REGULAR PLAY (TURNOVERS & BLOCKS) ---\n",
        "    {\n",
        "        \"id\": 3,\n",
        "        \"input_json\": '{\"time\": \"09:45\", \"team\": \"Fenerbah√ße\", \"player\": \"Scottie Wilbekin\", \"action\": \"turnover\", \"result\": \"steal\"}'\n",
        "    },\n",
        "    {\n",
        "        \"id\": 4,\n",
        "        \"input_json\": '{\"time\": \"08:12\", \"team\": \"Fenerbah√ße\", \"player\": \"Serta√ß ≈ûanlƒ±\", \"action\": \"block\", \"result\": \"success\"}'\n",
        "    },\n",
        "\n",
        "    # --- REBOUNDS (OFFENSIVE vs DEFENSIVE) ---\n",
        "    {\n",
        "        \"id\": 5,\n",
        "        \"input_json\": '{\"time\": \"05:14\", \"team\": \"Fenerbah√ße\", \"player\": \"Dyshawn Pierre\", \"action\": \"rebound\", \"result\": \"offensive\"}'\n",
        "    },\n",
        "    {\n",
        "        \"id\": 6,\n",
        "        \"input_json\": '{\"time\": \"06:30\", \"team\": \"Anadolu Efes\", \"player\": \"Tyrique Jones\", \"action\": \"rebound\", \"result\": \"defensive\"}'\n",
        "    },\n",
        "\n",
        "    # --- FOULS ---\n",
        "    {\n",
        "        \"id\": 7,\n",
        "        \"input_json\": '{\"time\": \"01:50\", \"team\": \"Fenerbah√ße\", \"player\": \"Johnathan Motley\", \"action\": \"shooting_foul\", \"result\": \"foul_out\"}'\n",
        "    },\n",
        "    {\n",
        "        \"id\": 8,\n",
        "        \"input_json\": '{\"time\": \"02:30\", \"team\": \"Fenerbah√ße\", \"player\": \"Melih Mahmutoƒülu\", \"action\": \"technical_foul\", \"result\": \"penalty\"}'\n",
        "    },\n",
        "\n",
        "    # --- FLOW & ASSISTS ---\n",
        "    {\n",
        "        \"id\": 9,\n",
        "        \"input_json\": '{\"time\": \"06:05\", \"team\": \"Anadolu Efes\", \"player\": \"Elijah Bryant\", \"action\": \"fast_break_layup\", \"result\": \"make\"}'\n",
        "    },\n",
        "    {\n",
        "        \"id\": 10,\n",
        "        \"input_json\": '{\"time\": \"03:15\", \"team\": \"Fenerbah√ße\", \"player\": \"Tyler Dorsey\", \"action\": \"drive_layup\", \"result\": \"miss\"}'\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "yELKL_OeG334"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Function"
      ],
      "metadata": {
        "id": "c0u9UnC1G-kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_tests_bilingual(model, tokenizer, test_data, label):\n",
        "    results = []\n",
        "    FastLanguageModel.for_inference(model)\n",
        "    print(f\"\\nüöÄ Running Bilingual Tests for: {label}\")\n",
        "\n",
        "    for item in test_data:\n",
        "        # We run the test TWICE for each row: Once for EN, Once for TR\n",
        "        languages = [\"English\", \"Turkish\"]\n",
        "\n",
        "        for lang_name in languages:\n",
        "\n",
        "            # 1. Construct Prompt (Using your proven format)\n",
        "            prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are a basketball commentator. Language: {lang_name}.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Input Data: {item['input_json']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\"\n",
        "            inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "            # 2. Generate\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=128,\n",
        "                use_cache=True,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                temperature=0.6\n",
        "            )\n",
        "\n",
        "            # 3. Decode & Clean\n",
        "            raw_output = tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
        "\n",
        "            if \"assistant\" in raw_output:\n",
        "                cleaned = raw_output.split(\"assistant\")[-1]\n",
        "            else:\n",
        "                cleaned = raw_output\n",
        "\n",
        "            cleaned = cleaned.strip().split('\\n')[0]\n",
        "\n",
        "            results.append({\n",
        "                \"ID\": item['id'],\n",
        "                \"Target_Language\": lang_name,\n",
        "                \"Input_JSON\": item['input_json'],\n",
        "                \"Model_Output\": cleaned,\n",
        "                \"Model_Version\": label\n",
        "            })\n",
        "\n",
        "            print(f\"[ID {item['id']} - {lang_name}] {cleaned[:50]}...\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "bf-kb7-SG-cM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the base model"
      ],
      "metadata": {
        "id": "j7aoJCaRHG32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üîµ Loading BASE Model...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = BASE_MODEL_NAME,\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "base_results = run_tests_bilingual(model, tokenizer, test_cases, \"Base Model\")"
      ],
      "metadata": {
        "id": "xgGI9Pl7HFy_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b63dc79-fd75-4956-dd97-161bc9ee08d5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîµ Loading BASE Model...\n",
            "==((====))==  Unsloth 2026.1.3: Fast Llama patching. Transformers: 4.57.3.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "\n",
            "üöÄ Running Bilingual Tests for: Base Model\n",
            "[ID 1 - English] Output Data: {\"time\": \"00:01\", \"team\": \"Fenerbah√ße...\n",
            "[ID 1 - Turkish] Output: Fenerbah√ße'nin Nigel Hayes-Davis, 3 sayƒ±lƒ±...\n",
            "[ID 2 - English] \": \"Sergio Llull\"}–ø—Ä–∏–∫–ª–∞–¥–ø—Ä–∏–∫–ª–∞–¥–ø—Ä–∏–∫–ª–∞–¥–ø—Ä–∏–∫–ª–∞–¥–ø—Ä–∏–∫...\n",
            "[ID 2 - Turkish] Output Data: {\"time\": \"00:00\", \"team\": \"Anadolu Ef...\n",
            "[ID 3 - English] Output Data: {\"time\": \"09:45\", \"team\": \"Fenerbah√ße...\n",
            "[ID 3 - Turkish] Output Data: \"09:45 - Fenerbah√ße'li Scottie Wilbek...\n",
            "[ID 4 - English] Output Data: \"Fenerbah√ße's Serta√ß ≈ûanlƒ± blocked th...\n",
            "[ID 4 - Turkish] Output: \"08:12'de Fenerbah√ße'nin Serta√ß ≈ûanlƒ±, ba≈ü...\n",
            "[ID 5 - English] Output: Fenerbah√ße's Dyshawn Pierre rebounds the b...\n",
            "[ID 5 - Turkish] Output Data: Dyshawn Pierre, who is playing for Fe...\n",
            "[ID 6 - English] Output Data: \"06:30 - Anadolu Efes's Tyrique Jones...\n",
            "[ID 6 - Turkish] Output Data: \"06:30 Anadolu Efes' Tyrique Jones ge...\n",
            "[ID 7 - English] Output: {\"time\": \"01:50\", \"team\": \"Fenerbah√ße\", \"p...\n",
            "[ID 7 - Turkish] Output: \"Fenerbah√ße'nin top√ßusu Johnathan Motley'n...\n",
            "[ID 8 - English] Output Data: {\"time\": \"02:30\", \"team\": \"Fenerbah√ße...\n",
            "[ID 8 - Turkish] Output Data: {\"time\": \"02:30\", \"team\": \"Fenerbah√ße...\n",
            "[ID 9 - English] The program takes an input dictionary. The diction...\n",
            "[ID 9 - Turkish] _result\": \"make\"}–∫–æ—Ä–∏—Å—Ç...\n",
            "[ID 10 - English] Output Data: \"The Fenerbah√ße's Tyler Dorsey drives...\n",
            "[ID 10 - Turkish] Output Data: \"Fenerbah√ße'nƒ±n Tyler Dorsey'nin driv...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test fine-tuned model"
      ],
      "metadata": {
        "id": "uadoW2R7HNtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nüü¢ Loading ADAPTER...\")\n",
        "model.load_adapter(ADAPTER_PATH)\n",
        "\n",
        "ft_results = run_tests_bilingual(model, tokenizer, test_cases, \"Fine-Tuned\")"
      ],
      "metadata": {
        "id": "XSg13nvNHSa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f3771ed-3dfd-4b7c-c7c4-07e9ee31aeac"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üü¢ Loading ADAPTER...\n",
            "\n",
            "üöÄ Running Bilingual Tests for: Fine-Tuned\n",
            "[ID 1 - English] Hayes-Davis is a superstar! He hits the game-winne...\n",
            "[ID 1 - Turkish] Hayes-Davis'ten m√ºthi≈ü bir ≈üut! Ma√ßta son isabet!...\n",
            "[ID 2 - English] Larkin's clutch play secures the win for Ef...\n",
            "[ID 2 - Turkish] Efes trib√ºnleri ayaƒüa kalktƒ±! Ma√ßƒ±n son dakikalarƒ±...\n",
            "[ID 3 - English] ...\n",
            "[ID 3 - Turkish] Wilbekin savunmayƒ±...\n",
            "[ID 4 - English] Not in his house! ≈ûannie sends it back with a mons...\n",
            "[ID 4 - Turkish] Devasa bir...\n",
            "[ID 5 - English] Pierre secures the rebound and puts it back in...\n",
            "[ID 5 - Turkish] Pierre...\n",
            "[ID 6 - English] ...\n",
            "[ID 6 - Turkish] Jones boyalƒ± alanda g√º√ßl√º bir pozisyonda, savunma ...\n",
            "[ID 7 - English] Motley picks up his fifth foul and is fouled out....\n",
            "[ID 7 - Turkish] Motley fa...\n",
            "[ID 8 - English] Paris steps to the line and calmly sinks the techn...\n",
            "[ID 8 - Turkish] Mahmutoƒülu faul yaptƒ±,...\n",
            "[ID 9 - English] Bryant draws...\n",
            "[ID 9 - Turkish] Bry...\n",
            "[ID 10 - English] Dorsey drives hard but gets rejected by the defens...\n",
            "[ID 10 - Turkish] Dorsey potaya sert gitti ama...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the results"
      ],
      "metadata": {
        "id": "I62C2W12HWqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_results = base_results + ft_results\n",
        "df = pd.DataFrame(all_results)\n",
        "df.to_csv(\"comparison_results.csv\", index=False)\n",
        "\n",
        "print(\"\\n‚úÖ DONE! Download 'comparison_results.csv' from the sidebar.\")\n",
        "print(df[[\"Target_Language\",\"Model_Version\", \"Model_Output\"]].head(4)) # Show comparison preview"
      ],
      "metadata": {
        "id": "aqM0q6VhHZKh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "368576ed-ba1e-4d2b-e34d-d5ca706cc98e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ DONE! Download 'comparison_results.csv' from the sidebar.\n",
            "  Target_Language Model_Version  \\\n",
            "0         English    Base Model   \n",
            "1         Turkish    Base Model   \n",
            "2         English    Base Model   \n",
            "3         Turkish    Base Model   \n",
            "\n",
            "                                        Model_Output  \n",
            "0  Output Data: {\"time\": \"00:01\", \"team\": \"Fenerb...  \n",
            "1  Output: Fenerbah√ße'nin Nigel Hayes-Davis, 3 sa...  \n",
            "2  \": \"Sergio Llull\"}–ø—Ä–∏–∫–ª–∞–¥–ø—Ä–∏–∫–ª–∞–¥–ø—Ä–∏–∫–ª–∞–¥–ø—Ä–∏–∫–ª–∞–¥...  \n",
            "3  Output Data: {\"time\": \"00:00\", \"team\": \"Anadol...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM as a Judge"
      ],
      "metadata": {
        "id": "_A_hhNYxkmmB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Table\n",
        "\n",
        "| ID | Language | Winner | FT Score (1-10) | Reasoning (Brief) |\n",
        "| :--- | :--- | :--- | :--- | :--- |\n",
        "| 1 | English | **Fine-Tuned** | 10 | FT captured the excitement and specific \"buzzer-beater\" context perfectly. Base failed by outputting JSON. |\n",
        "| 1 | Turkish | **Fine-Tuned** | 9 | FT used excellent local phrasing (\"son isabet\"). Base failed grammar (\"maalesef bir fail\"). |\n",
        "| 2 | English | **Fine-Tuned** | 8 | FT logic is good, though slightly cut off at the end. Base hallucinated \"Sergio Llull\" and outputted gibberish/Russian. |\n",
        "| 2 | Turkish | **Fine-Tuned** | 8 | FT tone is enthusiastic (\"trib√ºnleri ayaƒüa kalktƒ±\"), though cut off. Base outputted JSON/nonsense. |\n",
        "| 3 | English | **Fine-Tuned** | 1 | **Note:** FT output appears empty/missing in CSV. However, Base outputted JSON + Cyrillic (\"–ø—Ä–∏–∫–ª–∞–¥\"). FT wins by technicality of not outputting garbage, but both failed. |\n",
        "| 3 | Turkish | **Fine-Tuned** | 5 | FT started a correct sentence but was cut off. Base failed instruction (JSON dump). |\n",
        "| 4 | English | **Fine-Tuned** | 9 | Great idiomatic expression (\"Not in his house!\"). Base gave a meta-description of the data. |\n",
        "| 4 | Turkish | **Fine-Tuned** | 5 | Tone was good (\"Devasa bir\") but the output was severely truncated. Base failed instruction. |\n",
        "| 5 | English | **Fine-Tuned** | 9 | FT correctly inferred the \"put back\" from an offensive rebound. Base was robotic/meta. |\n",
        "| 5 | Turkish | **Fine-Tuned** | 4 | FT output was only one word (\"Pierre\"). Base outputted English text for the Turkish row. FT wins on language adherence. |\n",
        "| 6 | English | **Fine-Tuned** | 1 | FT output appears empty. Base outputted \"Output Data\" wrapper. Both failed, but Base violated negative constraints explicitly. |\n",
        "| 6 | Turkish | **Fine-Tuned** | 6 | FT context was correct (\"boyalƒ± alanda\") but sentence was incomplete. Base outputted JSON. |\n",
        "| 7 | English | **Fine-Tuned** | 10 | Perfect translation of \"foul_out\" to commentary. Base dumped JSON. |\n",
        "| 7 | Turkish | **Fine-Tuned** | 5 | FT started correctly but cut off. Base went into an infinite repetition loop. |\n",
        "| 8 | English | **Fine-Tuned** | 3 | **Hallucination Alert:** FT mentions \"Paris\" (Paris Lee?), but the JSON player is Melih Mahmutoƒülu. Base failed with JSON/Cyrillic loop. |\n",
        "| 8 | Turkish | **Fine-Tuned** | 7 | Accurate start to the sentence. Base failed with JSON/Cyrillic loop. |\n",
        "| 9 | English | **Fine-Tuned** | 6 | Accurate logic, slightly cut off. Base outputted a Python docstring explanation. |\n",
        "| 9 | Turkish | **Fine-Tuned** | 2 | FT output \"Bry\" (incomplete). Base outputted gibberish/code artifacts. |\n",
        "| 10 | English | **Fine-Tuned** | 9 | Good interpretation of the drive and miss. Base used \"The Fenerbah√ße's...\" (poor grammar) and meta-style. |\n",
        "| 10 | Turkish | **Fine-Tuned** | 8 | Natural phrasing (\"sert gitti ama\"). Base failed instruction (JSON dump). |\n",
        "\n",
        "---\n",
        "\n",
        "### Summary Report\n",
        "\n",
        "#### 1. Hallucination Rate\n",
        "*   **Base Model:** **Critical Failure.** The Base model hallucinated significantly, often generating Cyrillic text (Russian word \"–ø—Ä–∏–∫–ª–∞–¥\" meaning \"example\"), infinite loops (ID 7, 8), and phantom entities (\"Sergio Llull\" in ID 2). It also hallucinated English text when prompted for Turkish (ID 5).\n",
        "*   **Fine-Tuned Model:** **Low.** The Fine-Tuned model generally stuck to the entities provided in the JSON. However, it did exhibit one specific hallucination in **ID 8 (English)**, where it introduced a player named \"Paris\" who was not in the source JSON (likely confusing the context with an opponent player, Paris Lee).\n",
        "\n",
        "#### 2. Adherence to JSON Constraints & Instructions\n",
        "*   **Base Model:** **Fail.** The Base model consistently failed the negative constraint: \"Do not output JSON/Output Data.\" It almost exclusively outputted `Output Data: {JSON}` or meta-descriptions like \"The program takes an input dictionary.\"\n",
        "*   **Fine-Tuned Model:** **High.** The Fine-Tuned model successfully learned to strip the artifacts and output *only* the commentary. It successfully adopted the persona of a commentator rather than a data parser.\n",
        "\n",
        "#### 3. Quality of Turkish Translations & Formatting\n",
        "*   **Base Model:** Poor. It often treated Turkish prompts as English or outputted grammatically broken Turkish (e.g., \"maalesef bir fail\").\n",
        "*   **Fine-Tuned Model:** High Potential, but **Truncated**. The phrasing and idioms in Turkish were excellent and sounded like a real commentator (e.g., \"son saniyede g√∂nderiyor,\" \"isabet\"). However, the Fine-Tuned model suffers from a **Stop Token/Length issue**, as nearly 50% of its outputs were cut off mid-sentence (e.g., \"Motley fa\", \"Devasa bir\").\n",
        "\n",
        "**Conclusion:** The **Fine-Tuned Model** is the clear winner, transforming the raw data into exciting, idiomatic sports commentary. However, the model requires adjustment to its `max_tokens` or stop-sequence settings to prevent the frequent sentence truncation observed in the test set."
      ],
      "metadata": {
        "id": "4o_jEu8Bki4K"
      }
    }
  ]
}